{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import regex\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChristianBegriffe = [\"Hadoop\", \"Spark\", \"Cloudera\", \"Big+Data\", \"Kafka\", \"Elastic\", \"Hive\", \"Hortonworks\", \"Data+Engineer\", \"Data+Warehouse\", \"Datawarehouse\", \"DWH\", \"Data+Science\", \"Data+Scientist\", \"Python\", \"AWS\", \"GCP\", \"Pub/Sub\", \"Redshift\", \"Lift+&+Shift\", \"Google+Cloud\", \"Dataproc\", \"Datafusion\", \"Athena\", \"EMR\"]\n",
    "MichaelBegriffe = [\"Hadoop\", \"Big+Data\", \"Data+Engineer\", \"Data+Warehouse\", \"Datawarehouse\", \"DWH\", \"Business+Intelligence\", \"Business+Analytics\", \"Scrum+Master+Data\", \"Scrum+Master+Data+Analytics\", \"Data+Science\", \"Data+Scientist\", \"Machine+Learning\", \"Python\", \"Data+Vault\", \"Power+BI\", \"SSAS\", \"SSIS\", \"SSRS\", \"Hichert\", \"IBCS\", \"PowerQuery\", \"Azure+Analysis+Services\", \"Azure+Data+Factory\", \"Azure+Data+Lake\", \"Azure+SQL+DB\", \"Azure+Cognitive+Services\", \"Azure+ML+Services\", \"SQL+Services\", \"Data+Bricks\", \"MLOps\", \"Self-Service+BI\"]\n",
    "\n",
    "# AndreBegriffe = [\"Data Engineer\", \"Data Warehouse\", \"Datawarehouse\", \"DWH\", \"Data Vault\", \"Talend\", \"Salesforce\", \"DataStage\", \"WhereScape\", \"PostgrSQL\"]\n",
    "# FrankBegriffe = [\"Hadoop\", \"Big Data\", \"Hive\", \"Data Engineer\", \"Data Warehouse\", \"Datawarehouse\", \"DWH\", \"Datamanagement\", \"Data Quality\", \"Data governance\", \"Business Intelligence\", \"Business Analytics\", \"Business Analyse\", \"Business Analyst\", \"Product Owner Data\", \"Product Owner Data Analytics\", \"Scrum Master Data\", \"Scrum Master Data Analytics\", \"AI\", \"KI\", \"SAS\", \"Data Vault\", \"Self-Service BI\", \"ETL\"]\n",
    "# JannikBegriffe = [\"Data Engineer\", \"Data Warehouse\", \"Datawarehouse\", \"DWH\", \"Business Intelligence\", \"Business Analyse\", \"Business Analyst\", \"Product Owner Data\", \"Product Owner Data Analytics\", \"Requirements Engineer Data\", \"Requirements Engineer Data Analytics\", \"Scrum Master Data\", \"Scrum Master Data Analytics\", \"Power BI\", \"Hichert\", \"IBCS\", \"Tableau\"]\n",
    "# MichaelBegriffe = [\"Big Data\", \"Data Engineer\", \"Data Warehouse\", \"Datawarehouse\", \"DWH\", \"Business Intelligence\", \"Business Analytics\", \"Scrum Master Data\", \"Scrum Master Data Analystics\", \"Data Science\", \"Data Scientist\", \"Machine Learning\", \"Python\", \"Data Vault\", \"Power BI\", \"SSAS\", \"SSIS\", \"SSRS\", \"Hichert\", \"IBCS\", \"PowerQuery\", \"Azure Analysis Services\", \"Azure Data Factory\", \"Azure Data Lake\", \"Azure SQL DB\", \"Azure Cognitive Services\", \"Azure ML Services\", \"SQL Services\", \"Data Bricks\", \"MLOps\", \"Self-Service BI\"]\n",
    "# MichaelaBegriffe = [\"SAP BW\", \"SAP BI\", \"SAP Business Warehouse\", \"SAP Business Objects\", \"SAP BO\", \"SAP HANA\", \"Lumira\", \"BPC\"]\n",
    "# SvenBegriffe = [\"Business Analyse\", \"Business Analyst\", \"Scrum Master Data\", \"Scrum Master Data Analytics\", \"Data Science\", \"Data Scientist\", \"Machine Learning\", \"Python\", \"R\", \"R-Entwickler\", \"AI\", \"KI\", \"Azure Cognitive Services\", \"Azure ML Services\", \"MLOps\"]\n",
    "#usw.\n",
    "\n",
    "alle_fks = [(\"Christian\", ChristianBegriffe), (\"Michael\", MichaelBegriffe)] # hier auch die anderen einfügen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tauscht Begriffe in URL aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeURLHays(Begriffe): \n",
    "    url_list = []\n",
    "    i = 0\n",
    "    url = \"https://www.hays.de/jobsuche/stellenangebote-jobs/j/Contracting/3/c/Deutschland/D1641BCE-D56C-11D3-AFB2-00105AB00B48/p/1/?q=Hadoop&e=false\"\n",
    "    while(i < len(Begriffe) - 1):\n",
    "        url = url.replace(Begriffe[i], Begriffe[i+1])\n",
    "        # print(url)\n",
    "        i = i+1\n",
    "        url_list.append(url)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichert Ergebnislinks in Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinksHays(url): \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    links = []\n",
    "    for link in soup.findAll('a', class_ = 'search__result__header__a', attrs={'href': regex.compile(\"https://\")}): \n",
    "        links.append(link.get('href'))\n",
    "    return links\n",
    "# print(getLinksHays(\"https://www.hays.de/jobsuche/stellenangebote-jobs/j/Contracting/3/c/Deutschland/D1641BCE-D56C-11D3-AFB2-00105AB00B48/p/1/?q=Hadoop&e=false\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitleHays(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    title = soup.find('h1', class_ = 'hays__job__details__job__title').text\n",
    "    return title\n",
    "\n",
    "# print(getTitleHays(\"https://www.hays.de/jobsuche/stellenangebote-jobs-detail-devops-engineer-data-science-customer-relationship-management--crm-bonn-521743/1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diese Funktion kann auch für alle anderen Seiten aufgerufen werden (Gulp, Solcom etc.)\n",
    "# Als parameter wird die String-Ergebnisliste erwartet, die dann in das File geschrieben wird,\n",
    "# also: result_list = [\"Führungskraft\", \"Jobtitel\", \"Suchbegriff\", \"aktuelle Seite (z.b.Hays)\", \"Link zum Job\"]\n",
    "# am besten in genau dieser Reihenfolge, dann kann die Führungskraft auch für den File-Namen verwendet werden\n",
    "# Die Funkiton erstellt ein File \"results_Führunkgsraftxy\" und hängt die jeweiligen Ergebnisse immer an die bestehende Datei an.\n",
    "\n",
    "def saveResults(result_list):\n",
    "    filename = './jobboerse_{}.csv'.format(result_list[0])\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    with open (filename, 'a+') as csvfile:\n",
    "        headers = ['Für', 'Bezeichnung', 'Stichwort', 'Plattform', 'Datum', 'Link']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=';', lineterminator='\\n',fieldnames=headers)\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # file doesn't exist yet, write a header\n",
    "\n",
    "        writer.writerow({'Für': result_list[0], 'Bezeichnung': result_list[1], \n",
    "                     'Stichwort':result_list[2],'Plattform':result_list[3], \n",
    "                         'Datum':result_list[4], 'Link':result_list[5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fk, begriffe in alle_fks:\n",
    "    print(fk)\n",
    "    i = 0\n",
    "    date = (datetime.date.today()).strftime('%d-%m-%Y')\n",
    "    # links for keywords\n",
    "    hays_urls = changeURLHays(begriffe)\n",
    "    \n",
    "    # get joblinks for every keyword\n",
    "    for u in hays_urls:\n",
    "        i += 1\n",
    "        hays_job_links = getLinksHays(u)\n",
    "        print(len(hays_job_links))\n",
    "    \n",
    "        # get title for every joblink\n",
    "        for l in hays_job_links:\n",
    "            title = getTitleHays(l)\n",
    "            hays_result_list = [str(fk), title, str(begriffe[i]), \"hays\", date, l]\n",
    "            saveResults(hays_result_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gulp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tauscht Begriffe in URL aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeURLGulp(Begriffe): \n",
    "     i = 0\n",
    "     url = \"https://www.gulp.de/gulp2/g/projekte?scope=projects&query=Hadoop&order=DATE_DESC&region=D0&region=D1&region=D2&region=D3&region=D4&region=D5&region=D6&region=D7&region=D8&region=D9\"\n",
    "     while(i < len(Begriffe)): \n",
    "        url = url.replace(Begriffe[i], Begriffe[i+1])\n",
    "        print(url)\n",
    "        i = i+1\n",
    "changeURLGulp(Begriffe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichert Ergebnislinks in Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "def getLinksGulp(url): \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    links = []\n",
    "    baseUrl = \"https://gulp.de\"\n",
    "    for link in soup.findAll('a', href=True): \n",
    "        newLink = baseUrl.join(link)\n",
    "        #links.append(link.get('href'))\n",
    "    return newLink\n",
    "print(getLinksGulp(\"https://www.gulp.de/gulp2/g/projekte?scope=projects&query=Hadoop&order=DATE_DESC&region=D0&region=D1&region=D2&region=D3&region=D4&region=D5&region=D6&region=D7&region=D8&region=D9\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder, not finished yet\n",
    "for fk, begriffe in alle_fks:\n",
    "    i = 0\n",
    "    \n",
    "    # links for keywords\n",
    "    gulp_urls = changeURLGulp(begriffe)\n",
    "    \n",
    "    # get joblinks for every keyword\n",
    "    for u in gulp_urls:\n",
    "        i += 1\n",
    "        gulp_job_links = getLinksGulp(u)\n",
    "    \n",
    "        # get title for every joblink\n",
    "        for l in gulp_job_links:\n",
    "            title = getTitleHays(l)\n",
    "            gulp_result_list = [str(fk), title, str(begriffe[i]), \"gulp\", l]\n",
    "            saveResults(gulp_result_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solcom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benutze Google Chrome und es ist lokal gespeichert\n",
    "\n",
    "driver = webdriver.Chrome('/Users/vonderheyden/Downloads/chromedriver_win32/chromedriver.exe')\n",
    "driver.get('https://www.solcom.de/de/projektportal/projektangebote')\n",
    "\n",
    "# Stellt die Standardfilter ein\n",
    "def editFilter(): \n",
    "    driver.find_element_by_id(\"keyword\").send_keys(\"Hadoop\") # Default: Erster Begriff\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_class_name(\"prio1.acceptall\").click()\n",
    "    driver.find_element_by_class_name(\"current-selection\").click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"main\"]/div/div/div/div/div/div[1]/div[1]/form/div[3]/div/div[2]/ul/li[2]/div/ins').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"main\"]/div/div/div/div/div/div[1]/div[1]/form/div[3]/div/div[2]/ul/li[5]/div[1]/ins').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"main\"]/div/div/div/div/div/div[1]/div[1]/form/button[1]').click()\n",
    "\n",
    "editFilter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Westhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/Users/vonderheyden/Downloads/chromedriver_win32/chromedriver.exe')\n",
    "driver.get('https://www.westhouse-group.com/en/for_candidates')\n",
    "\n",
    "# Stellt die Standardfilter ein\n",
    "# F oder P muss bei der aktiven Informationsextraktion gefiltert werden\n",
    "def editFilter(): \n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath('//*[@id=\"CookieBoxSaveButton\"]').click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('job-finder-search-keyword').send_keys(\"Hadoop\")\n",
    "    driver.find_element_by_xpath('//*[@id=\"headingjobtype2\"]/p/a/span/i[1]').click()\n",
    "\n",
    "editFilter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freelancermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gefiltert ohne Hadoop\n",
    "# driver.get(\"https://www.freelancermap.de/projektboerse.html?profisuche=1&pq_vertragsart%5Bcontract_type.remote%5D=REMOTE&pq_projekteinstellung=0&pq_sorttype=1&mCats%5B0%5D=1&mCats%5B1%5D=2&mCats%5B2%5D=3&mCats%5B3%5D=4&mCats%5B4%5D=5&mCats%5B5%5D=6&sCou%5B0%5D=1\")\n",
    "\n",
    "Begriffe = [\"Hadoop\", \"Spark\", \"Cloudera\", \"Big+Data\", \"Kafka\", \"Elastic\", \"Hive\", \"Hortonworks\", \"Data+Engineer\", \"Data+Warehouse\", \"Datawarehouse\", \"DWH\", \"Data+Science\", \"Data+Scientist\", \"Python\", \"AWS\", \"GCP\", \"Pub/Sub\", \"Redshift\", \"Lift+%26+Shift\", \"Google+Cloud\", \"Dataproc\", \"Datafusion\", \"Athena\", \"EMR\"]"
   ]
  },
  {
   "source": [
    "Begriffe austauschen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeURLFreelancer(Begriffe): \n",
    "    i = 0\n",
    "    url = \"https://www.freelancermap.de/?module=projekt&func=suchergebnisse&pq=Hadoop&profisuche=0&pq_sorttype=1&mCats[]=1&mCats[]=2&mCats[]=3&mCats[]=4&mCats[]=5&mCats[]=6&redirect=1\"\n",
    "    while(i < len(Begriffe) - 1): \n",
    "        url = url.replace(Begriffe[i], Begriffe[i+1])\n",
    "        print(url)\n",
    "        i = i+1\n",
    "changeURLFreelancer(Begriffe)"
   ]
  },
  {
   "source": [
    "Titel extrahieren"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitlesFreelancer(url): \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    titles = []\n",
    "    for title in soup.findAll('li', class_ = 'project-row'): \n",
    "        titles.append(title.find('a').getText())\n",
    "    titles = titles[5:]\n",
    "    return titles\n",
    "print(getTitlesFreelancer(\"https://www.freelancermap.de/?module=projekt&func=suchergebnisse&pq=Hadoop&profisuche=0&pq_sorttype=1&mCats[]=1&mCats[]=2&mCats[]=3&mCats[]=4&mCats[]=5&mCats[]=6&redirect=1\"))"
   ]
  },
  {
   "source": [
    "Links extrahieren"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['/projektboerse/projekte/it/2057007-projekt-senior-engineer-business-functional-analyst-project-manager-solution-architect-bonn.html', '/projektboerse/projekte/it/2056590-projekt-nifi-developer-d-darmstadt.html', '/projektboerse/projekte/it/2056341-projekt-apache-nifi-developer--6-months--remote-germany--start-asap-deutschland.html', '/projektboerse/projekte/it/2056326-projekt-system-engineer-architect-m-f-d-wanted-cpat2302212-berlin.html', '/projektboerse/projekte/it/2056280-projekt-pyspark-scala-muelheim.html', '/projektboerse/projekte/it/2056275-projekt-apache-nifi-developer-english-remote-frankfurt.html', '/projektboerse/projekte/it/2056269-projekt-nifi-developer-remote-or-berlin.html', '/projektboerse/projekte/it/2056200-projekt-apache-nifi-developer-english-remote-frankfurt.html', '/projektboerse/projekte/it/2056182-projekt-pyspark-scala-lead-expert-frankfurt-germany.html', '/projektboerse/projekte/it/2056143-projekt-data-engineer-with-cloudera-experience-freelance-100-remote-german-speaking-anywhere.html', '/projektboerse/projekte/it/2056134-projekt-job-opportunity-for-senior-business-functional-analyst-ifrs-9-eschborn.html', '/projektboerse/projekte/it/2055307-projekt-pyspark-scala-engineer-developer-munich.html', '/projektboerse/projekte/it/2055281-projekt-system-engineer-munich.html', '/projektboerse/projekte/it/2054411-projekt-pyspark-scala-hadoop-expert-mulheim.html', '/projektboerse/projekte/it/2053295-projekt-pyspark-scala-expert-mulheim.html', '/projektboerse/projekte/it/2053294-projekt-pyspark-scala-expert-mulheim.html', '/projektboerse/projekte/it/2053233-projekt-projektanfrage-system-engineer-architect-w-m-d-berlin-remote-asap-4703-berlin.html', '/projektboerse/projekte/it/2053169-projekt-system-engineer-6-month-remote-freelance-job-berlin.html', '/projektboerse/projekte/it/2050937-projekt-remote-data-engineer-berlin.html', '/projektboerse/projekte/it/2050332-projekt-cloud-big-data-engineer-100-remote-german-speaking-initial-3-month-contract-likely-extension-anywhere.html', '/projektboerse/projekte/it/2048405-projekt-data-engineer-remote.html', '/projektboerse/projekte/it/2047734-projekt-data-intelligence-engineer-6-month-contract-remoteberlin.html', '/projektboerse/projekte/it/2047043-projekt-data-engineer-sql-hannover.html', '/projektboerse/projekte/it/2043209-projekt-freelance-data-engineer-architect-hamburg-berlin-munich-cologne-frankfurt-stuttgart-dusseldorf-dortmund-essen-leipzig.html', '/projektboerse/projekte/it/2042260-projekt-senior-data-specialist-machine-learning-asap-gesucht-100-remote-nrw.html']\n"
     ]
    }
   ],
   "source": [
    "def getLinksFreelancer(url): \n",
    "    prefix = \"https://www.freelancermap.de\" \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    links = []\n",
    "    for link in soup.findAll('a', attrs={'href': regex.compile(\"/projektboerse/projekte/it/\")}): \n",
    "        links.append(link.get('href'))\n",
    "        #DEBUG\n",
    "    for link in links: \n",
    "        link.replace(link, prefix + link)\n",
    "    return links\n",
    "print(getLinksFreelancer(\"https://www.freelancermap.de/?module=projekt&func=suchergebnisse&pq=Hadoop&profisuche=0&pq_sorttype=1&mCats[]=1&mCats[]=2&mCats[]=3&mCats[]=4&mCats[]=5&mCats[]=6&redirect=1\"))"
   ]
  },
  {
   "source": [
    "Projekt-ID extrahieren"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<div class=\"project-detail-description\">\n                \n                                                        Remote\n                            </div>\n"
     ]
    }
   ],
   "source": [
    "def getProjectID(link): \n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    return soup.find('div', class_ = \"project-detail-description\")\n",
    "\n",
    "print(getProjectID(\"https://www.freelancermap.de/projektboerse/projekte/it/2057007-projekt-senior-engineer-business-functional-analyst-project-manager-solution-architect-bonn.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}